---
title: "The College Scorecard"
author: "Varun Nadgir"
date: "September 27, 2017"
output:
  html_document: default
  pdf_document: default
---

<style>
body {text-align: justify}
p {text-indent: 50px}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Capstone Project

### Introduction

The [**College Scorecard**](https://collegescorecard.ed.gov/) is a service meant to help prospective students make their college decision. Whether by comparing size, popular majors, or comparing costs to the national average, the site's goal is to help the user find a good fit. For my Springboard Capstone Project, I used the [**dataset**](https://catalog.data.gov/dataset/college-scorecard) made available by the College Scorecard to try and find additional ways to help users in their decision. 

### The Data

Available in a .zip file from the link above, the data is split into 19 .csv files - one for each academic year from 1996-'97 to 2014-'15. Each file contains 1,744 recorded data points (columns) and about 7,500 schools (rows). My first step was to add a DATAYEAR column to indicate the academic year and then merging the 19 files into one large .csv, which I called **fulldata.csv**. From this 2 GB file, I would create subsets for plotting and studying trends. Next, I had to refer to [**the data dictionary**](https://collegescorecard.ed.gov/data/documentation/) to understand the column names and some of the placeholder values used. Once I merged the files and had a basic understanding of what data was available, I began to create some plots and documented my initial findings in my [**data story**](http://varun.pro/Springboard/DataStory/collegescorecard.html). As a note, my studies have been on US schools only. The dataset includes records of US territories as well, but I have filtered them out when creating subsets for plotting/modeling. 

### Deliverables

My first item will be using linear regression models to determine what variables are the most influential on the cost of a school, and to potentially predict what the cost of a school may be in the future. This would be useful in two ways. It could help students who are on the fence about going to college immediately after high school by suggesting a decline in cost. If a student sees that their ideal school is likely to be cheaper in two years, they may make the decision to find entry-level work or go traveling before going to college. It could also help the schools by indicating areas of their budget that are influencing the cost of attendance. Of course, the goals of each school are different and they may not be interested in reducing cost, but if a school is experiencing a decline in applications, cost of attendance is likely to be something they look at. 

The second item is a recommendation tool that works similarly to clustering methods used by media services such as Netflix, YouTube, and iTunes. Clustering based on things like location, cost, and SAT scores, a student can find options that are close to their top choice. Mentality is a very important part of finding success at school, and feeling out of place in freshman year can be quite discouraging. If their top choice is a far reach school, or it is too expensive/too far, then finding alternatives would hopefully help them to be satisfied in their decision.

My final item will be a basic UI that allows the user to explore the dataset on their own. Although the data will need to be curated and shaved down to a size that a standard internet browser can handle, my hope is that it will provide some transparency between students and universities. As an example, in my [**data story**](http://varun.pro/Springboard/DataStory/collegescorecard.html), I explored SAT averages and admission rates. Even though one could reasonably guess how they are related (higher SAT scores ~ lower admission rates), being able to plot the data and draw a conclusion from a graph is much more convincing. By putting this power in the hands of students and their families, they should be able to make much more educated decisions.

### Data Preparation

From **fulldata.csv**, I created and saved a subset of the columns that would make repeated data manipulation more manageable. I named it **subdata.csv** and it contains various *school identification data* (name, ID, location, level of institution), *admission data* (student demographics, SAT scores), *education data* (majors, completion rates), and *financial data* (cost, aid, debt, repayment). 

From this subset, I can pick the variables for the cost modeling, as well as the variables for clustering - both tools will get specific subsets made so that the data is organized and loading it will be easier in the future. A combination of the original source data and my curated data will go into the UI plotting tool, keeping in mind that the dataset should stay a reasonable size but also have enough depth for the user to gain whatever insights they can.

### Cost Prediction

For convenience, I began by changing the DATAYEAR column from indicating the year range ("1996-'97") to simply having the start year ("1996"). Using the datayear, I created the training file using years 1996 to 2013 and the test file with just the 2014 data. Since the cost is known for 2014, we can compare afterwards to see how good the model is beyond the $R^2$ score alone.

From **subdata.csv**, I created a subset of up to 14 columns that I felt would be related to the attendance cost of a school. This included columns like "CONTROL" (whether the school is public or private), "UGDS" (the undergraduate student population), "PFTFAC" (percentage of faculty that are full time), and of course "COSTT4_A" (cost of attendance for a 4-year academic year school). Upon checking the summary() of the training set, I noticed that the MAX of "UGDS" was unnaturally high. This was because there were a few records of the University of Pheonix's online campus, which is capable of having about 200,000 students. Since this was skewing the data, I removed its related "OPEID6" from the training and test sets. I also removed all records in the training set that did not have a "COSTT4_A" entry, since they would not contribute to the model. This filtering had an interesting byproduct - apparently years 2008 and prior did not have "COSTT4_A" records, so removing the NA entries also happened to filter the dataset to years 2009 and beyond. After taking care to turn appropriate columns into the numeric data type, I moved on to make small experimental models.

The first model I tried was on Cost by Undergrad Pop., Average Faculty Salary, and % of Faculty that are Full-Time. This, however, only gave an $R^2$ of 0.2198, which is not that great. However, the model summary suggests that these three variables are still significant (using asterisk notation), so it is a step in the right direction. The 2nd model I tried used 12 independent variables to model Cost. It gave an $R^2$ of 0.8253, but still showed that three of the variables had little to no significance. After removing these, this was the final model I came to:

#### Cost by Datayear, Highest degree offered, Pub/Priv, Avg SAT, Undergrad Pop., Avg Faculty Salary, % of Faculty that are Full-Time, Median Debt, and Avg Family Income

```{r costmodel, message=FALSE, warning=FALSE, fig.align="center"}
# load libraries
library(readr)
library(tidyr)

df.train <- read_csv("data/df_train.csv")

cost.mod <- lm(COSTT4_A ~ DATAYEAR + HIGHDEG + CONTROL + SAT_AVG + UGDS + AVGFACSAL + PFTFAC + DEBT_MDN + FAMINC, data = df.train, na.action = na.omit)

summary(cost.mod)

```

With an $R^2$ of 0.825, this model is almost equally as good as the second model, but now all of the variables used are significant. In other words, the variables that influence the cost of attendance of a school are the ones in this model. This is saved as **cost.mod** and will be used to predict on the test set. Before predicting, however, we should take a look at the plot of this model so that we get some visual context on how good or bad this fit is.

#### Plot of cost.mod

```{r modelsumm, message=FALSE, warning=FALSE, fig.align="center"}
# load libraries
library(readr)
library(tidyr)

df.train <- read_csv("data/df_train.csv")

cost.mod <- lm(COSTT4_A ~ DATAYEAR + HIGHDEG + CONTROL + SAT_AVG + UGDS + AVGFACSAL + PFTFAC + DEBT_MDN + FAMINC, data = df.train, na.action = na.omit)

plot(cost.mod)
```

The Residuals vs Fitted plot stands out right away, showing that the residuals generally reside around the fitted line. The Normal Q-Q plot also follows the normal line for the most part, aside from the very bottom and very top of the plot. It seems like this model is fairly strong, and the $R^2$ and plot of residuals reinforces it. Now we can make some predictions on the 2014 data and make some comparisons.

#### Summary of test data after predictions

```{r costpredict, message=FALSE, warning=FALSE, fig.align="center"}
# load libraries
library(readr)

df.test <- read_csv("data/cost_predict.csv")

summary(df.test)
```

The predict() formula returns a column that I named "PREDICT" and, by taking the difference of "PREDICT" and "COSTT4_A", we can see whether the prediction was too high or too low. This summary tells us that the extreme cases were -\$15,605 and \$26,866 off the mark. What is more disappointing, however, is the number of NAs in the prediction output. To see where the model is falling short, I can check two plots. The first plot overlays the prediction values (in black) over the training data. This helps to see if the predicted values stayed in the same range and may indicate another reason why some predictions weren't made. The second plot overlays the prediction values over the actual 2014 values, and they are coloured green if within $4000 of the actual cost and red if predicted too far. If the plot is mostly green, that would be a good sign.

```{r predictplot1, message=FALSE, warning=FALSE, fig.align="center"}
# load libraries
library(readr)
library(ggplot2)


df.test <- read_csv("data/cost_predict.csv")

# plot training set (see costs by ID) and plot the predictions over it (see how the spread looks)
plot1 <- ggplot(NULL, aes(OPEID6, COSTT4_A)) + 
  geom_point(data = df.train, aes(col = as.factor(REGION))) + 
  geom_point(data = df.test, aes(y = df.test$PREDICT)) +
  labs(x = "OPEID6", y = "COSTT4_A", 
       title = "Prediction (Black) vs. Training (Coloured)") + 
  scale_colour_discrete(name = "Region", 
                        labels = c("US Service Schools", "New England", "Mid East", "Great Lakes", "Plains", "Southeast", "Southwest", "Rocky Mountains", "Far West", "Outlying Areas"))

# plot predictions over the actual for 2014
plot2 <- ggplot(NULL) + 
  geom_point(data = df.test, aes(df.test$OPEID6, df.test$COSTT4_A), 
             col = "black") + 
  geom_point(data = df.test, aes(df.test$OPEID6, df.test$PREDICT), 
             col = ifelse(abs(df.test$DIFF) <= 4000, "green", "red")) +
  labs(x = "OPEID6", y = "COSTT4_A",
       title = "Prediction (Red/Green) vs. Actual (Black)")

plot1
plot2

```

According to these plots, the predictions were mostly succesful for schools in the 0 ~ 3000 range for "OPEID6", while predictions beyond that are very sporadic. Looking at the dataset and sorting by "OPEID6" shows that there are many NAs in the independent variables, causing the prediction to fail for those cases. This is unfortunate, since this causes about 2,600 failed predictions despite a fairly strong model. This leaves us with a few different options. We could remove the independent variable(s) with the most NAs from the model, which risks weakening the model but will yield more successful predictions. Another option could be to replace NAs with another value, such as the mean for that column. This doesn't hurt the model at all, but the predictions for the schools with approximated values have a chance to be completely off.

The independent variable with significantly more NAs than the others is "SAT_AVG". We can create a new model that does not include "SAT_AVG" and, it turns out, removing it reveals "PFTFAC" to be an insignificant variable as well. With both removed, the new model has an $R^2$ of 0.771, which is not bad at all. Checking the summary of this new model shows that there are only about 400 NAs in the prediction this time, which is a massive improvement. 

```{r temptest, message=FALSE, warning=FALSE, fig.align="center"}
# load libraries
library(readr)
library(tidyr)

df.train <- read_csv("data/df_train.csv")

mod1 <- lm(COSTT4_A ~ DATAYEAR + HIGHDEG + CONTROL + UGDS + AVGFACSAL + DEBT_MDN + FAMINC, data = df.train, na.action = na.omit)

summary(mod1)
```

We can compare the performance of this model to the other option, where NAs would have been replaced by the column mean. Is it worth it to have a slightly weaker model for more accurate predictions, or will replacing the NAs solve the problem? Ideally, the colour coding of the plot will indicate the better model.

```{r comparemod, message=FALSE, warning=FALSE, fig.align="center"}
# load libraries
library(readr)
library(ggplot2)

temp.test <- read_csv("data/temp_test.csv")
df.test.approx <- read_csv("data/cost_predict_approx.csv")

# plot temp predictions over the actual for 2014
ggplot(NULL) + 
  geom_point(data = temp.test, aes(temp.test$OPEID6, temp.test$COSTT4_A), 
             col = "black") + 
  geom_point(data = temp.test, aes(temp.test$OPEID6, temp.test$PREDICT), 
             col = ifelse(abs(temp.test$DIFF) <= 4000, "green", "red")) +
  labs(x = "OPEID6", y = "COSTT4_A",
       title = "Weaker Model, Fewer NAs")

# plot temp predictions over the actual for 2014
ggplot(NULL) + 
  geom_point(data = df.test.approx, aes(df.test.approx$OPEID6, df.test.approx$COSTT4_A), 
             col = "black") + 
  geom_point(data = df.test.approx, aes(df.test.approx$OPEID6, df.test.approx$PREDICT), 
             col = ifelse(abs(df.test.approx$DIFF) <= 4000, "green", "red")) +
  labs(x = "OPEID6", y = "COSTT4_A",
       title = "Original Model, NAs replaced")

```



### School Recommendation



### College Scorecard Sandbox

